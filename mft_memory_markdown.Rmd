---
title: "MFT Memory"
output:
  html_document:
    df_print: paged
---

This is a project testing memory for vignettes of moral transgressions. 

# Design

Participants (n = 28) incidentally encoded 120 vignettes, 15 from each of the different foundations articulated by Moral Foundations Theory (cite), by reading and vividly imagining the vignette for 6 seconds and then making a moral judgement. 

During recognition, participants were cued with each vignette presented during study + 7-8 lures per foundation. Each vignette had up to 4 words removed, and participants were asked to select which word or words completed the vignette as they studied it as well as their confidence in their response. 

Encoding took place in the scanner but retrieval did not. We plan to conduct a PLS analysis on the neural data, but are getting started with analyzing behavior first. 

```{r echo=FALSE}
# load packages
library(knitr)
library(ggplot2)
library(psych)
library(lme4)
library(lmerTest)
library(ggpubr)
library(tidyr)
library(ggeffects)
library(tidyselect)
library(dplyr)
se <- function(x) sqrt(var(x)/length(x))
`%notin%` <- Negate(`%in%`)
```


```{r echo=FALSE}

# load encoding data
encodingfiles = list.files('C:/Users/imclab/Desktop/MFT re-analysis/data/encoding', full.names = TRUE, pattern = '.csv', recursive = FALSE)
encodingFull = do.call(rbind, lapply(encodingfiles, function(x){read.csv(x, header = TRUE, stringsAsFactors = FALSE)}))

# load retrieval data
retfiles = list.files('C:/Users/imclab/Desktop/MFT re-analysis/data/retrieval', full.names = TRUE, pattern = '.csv', recursive = FALSE)
retFull = do.call(rbind, lapply(retfiles, function(x){read.csv(x, header = TRUE, stringsAsFactors = FALSE)}))

# load vignette key
key <- read.csv('C:/Users/imclab/Desktop/MFT re-analysis/key.csv', stringsAsFactors = FALSE)

# load qualtrics
foundation_matching <- read.csv('C:/Users/imclab/Desktop/MFT re-analysis/data/qualtrics/foundation_matching_all.csv', stringsAsFactors = FALSE)
emorate <- read.csv('C:/Users/imclab/Desktop/MFT re-analysis/data/qualtrics/emorate.csv', stringsAsFactors = FALSE)
all_surveys <- read.csv('C:/Users/imclab/Desktop/MFT re-analysis/data/qualtrics/all_surveys.csv', stringsAsFactors = FALSE)
```


```{r echo=FALSE}

# tidy encoding
envars <- names(encodingFull) %in% c('run','pres_order','vignum','trialstart_raw','trialstart_zeroed','trialstart_TR','trialdur','intended_jitter','ITI_start_raw','ITI_start_zeroed','ITI_start_TR','ITI_duration','ITI_accuracy', 'foundation', 'vigtext') 
encoding_behav_all <- encodingFull[!envars]
encoding_behav_all <- filter(encoding_behav_all, vigfile != 99999)
encoding_behav_all <- as.data.frame(sapply(encoding_behav_all, function(x) gsub(".jpg", "", x)), stringsAsFactors = FALSE) #get rid of '.jpg' suffix
names(encoding_behav_all)[4]<-"encoding_RT"
encoding_behav_all <- as.data.frame(sapply(encoding_behav_all, function(x) gsub("^\t* *\"* *| *\"* *\t*$", "", x)), stringsAsFactors = FALSE) #get rid of quotation marks & any white space

# tidy key
key <- as.data.frame(sapply(key, function(x) gsub("^\t* *\"* *| *\"* *\t*$", "", x)), stringsAsFactors = FALSE)
key <- subset(key, select = -c(correct_answer, choice1, choice2, choice3)) #delete extra columns

# tidy retrieval
ret_all <- retFull
ret_all$resp <- sapply(ret_all$resp, function(s) as.numeric(substr(s, 1, 2))) #get rid of symbols
ret_all$confresp <- sapply(ret_all$confresp, function(s) as.numeric(substr(s, 1, 2))) #get rid of symbols
ret_all <- as.data.frame(sapply(ret_all, function(x) gsub("^\t* *\"* *| *\"* *\t*$", "", x)), stringsAsFactors = FALSE) #get rid of quotation marks & any white space
ret_all <- left_join(ret_all, key) #make column with category for each cue
ret_all$old <- as.integer(substr(ret_all$category, 1,4) != 'Lure') #add old/new status of each item
ret_all <- as.data.frame(sapply(ret_all, function(x) gsub("Lure-", "", x)), stringsAsFactors = FALSE) #get rid of 'Lure-' prefix
ret_all$vigfile[ret_all$vigfile == ''] = NA
names(ret_all)[8]<-"retrieval_RT"

# one big happy dataframe
encoding_behav <- subset(encoding_behav_all, encoding_behav_all$subID %in% ret_all$subID)
ret <- subset(ret_all, ret_all$subID %in% encoding_behav$subID)
data <- merge(ret, encoding_behav, by=c('subID', 'vigfile'), all.x = TRUE)

# homogenize variable names
data$category[data$category == "SN"] <- "Social-Norms"
data$category[data$category == "Lib"] <- "Liberty"
data$category[data$category == "Loy"] <- "Loyalty"
data$category[data$category == "Pure"] <- "Purity"
data$category[data$category == "Auth"] <- "Authority"
data$category[data$category == "Fair"] <- "Fairness"
data$category[data$category == "Social-Norms"] <- "Social Norms"

# make text-based response column; new responses = NA
data$text_response = ''
i = 0
for (n in data$resp) {
  i = i + 1
  if (is.na(n) == TRUE) {                       
    data$text_response[i] = NA
  }
  else if (n == 1) {
    data$text_response[i] = data$choice1[i]
  }
  else if (n == 2) {
    data$text_response[i] = data$choice2[i]
  }
  else if (n == 3) {
    data$text_response[i] = data$choice3[i]
  }
  else {
    data$text_response[i] = NA
  }
} 

# make column for calculating proportion correct
data$correct <- as.numeric((!is.na(data$text_response) & data$text_response == data$correct_answer) |
  (is.na(data$text_response) & data$old == 0))

# make column for calculating corrected recognition
data$SDT[data$old == 1 & data$correct == 1] = 'hit'
data$SDT[data$old == 0 & data$correct == 0] = 'falseAlarm'
data$SDT[data$old == 1 & data$correct == 0] = 'miss'
data$SDT[data$old == 0 & data$correct == 1] = 'correctRejection'

# make column for calculating confidence
data$high_conf[data$confresp == 1 | data$confresp == 2] = 0
data$high_conf[data$confresp == 3 | data$confresp == 4] = 1


## analysis with only retrieval sample
ret_all$category[ret_all$category == "SN"] <- "Social-Norms"
ret_all$category[ret_all$category == "Lib"] <- "Liberty"
ret_all$category[ret_all$category == "Loy"] <- "Loyalty"
ret_all$category[ret_all$category == "Pure"] <- "Purity"
ret_all$category[ret_all$category == "Auth"] <- "Authority"
ret_all$category[ret_all$category == "Fair"] <- "Fairness"
ret_all$category[ret_all$category == "Social-Norms"] <- "Social Norms"

# make text-based response column; new responses = NA
ret_all$text_response = ''
i = 0
for (n in ret_all$resp) {
  i = i + 1
  if (is.na(n) == TRUE) {                       
    ret_all$text_response[i] = NA
  }
  else if (n == 1) {
    ret_all$text_response[i] = ret_all$choice1[i]
  }
  else if (n == 2) {
    ret_all$text_response[i] = ret_all$choice2[i]
  }
  else if (n == 3) {
    ret_all$text_response[i] = ret_all$choice3[i]
  }
  else {
    ret_all$text_response[i] = NA
  }
}


# correct column
ret_all$correct <- as.numeric((!is.na(ret_all$text_response) & ret_all$text_response == ret_all$correct_answer) |
  (is.na(ret_all$text_response) & ret_all$old == 0))

# corrected recognition column
ret_all$SDT[ret_all$old == 1 & ret_all$correct == 1] = 'hit'
ret_all$SDT[ret_all$old == 0 & ret_all$correct == 0] = 'falseAlarm'
ret_all$SDT[ret_all$old == 1 & ret_all$correct == 0] = 'miss'
ret_all$SDT[ret_all$old == 0 & ret_all$correct == 1] = 'correctRejection'

# make column for calculating proportion high confidence responses
ret_all$high_conf[ret_all$confresp == 1 | ret_all$confresp == 2] = 0
ret_all$high_conf[ret_all$confresp == 3 | ret_all$confresp == 4] = 1

# make column for moral judgement x memory accuracy correlation
ret_all$hits[ret_all$old == 1 & ret_all$correct == 1] = 1
ret_all$hits[ret_all$old == 1 & ret_all$correct == 0] = 0

data$hits[data$old == 1 & data$correct == 1] = 1
data$hits[data$old == 1 & data$correct == 0] = 0



# separate questionnaires
mfq <- select(all_surveys, starts_with('sub'), starts_with('mfq'))
disgust_scale <- select(all_surveys, starts_with('sub'), starts_with('disgust'))
secs <- select(all_surveys, starts_with('sub'), starts_with('secs'))
iri <- select(all_surveys, starts_with('sub'), starts_with('iri'))

# check to see which ppts I don't have qualtrics & behavioral data for 
test <- data %>% 
        group_by(subID) %>%
        summarise(confmean = mean(as.numeric(confresp), na.rm = TRUE))
all_surveys$subID <- as.numeric(all_surveys$subID)
test$subID %in% all_surveys$subID
# ppts in test that are not in qualtrics: 23, 7; 20 is in there but they stopped responding after MFQ
# there's 1 subject whose id was 'LP' that I think is ppt 7 but need to do further digging to verify

# score MFQ
#part one
mfq[mfq == 'Not at all relevant' ] <- 0
mfq[mfq == 'Not very relevant' ] <- 1
mfq[mfq == 'Slightly relevant' ] <- 2
mfq[mfq == 'Somewhat relevant' ] <- 3
mfq[mfq == 'Very relevant' ] <- 4
mfq[mfq == 'Extremely relevant' ] <- 5
#part two
mfq[mfq == 'Strongly disagree' ] <- 0
mfq[mfq == 'Moderately disagree' ] <- 1
mfq[mfq == 'Slightly disagree' ] <- 2
mfq[mfq == 'Slightly agree' ] <- 3
mfq[mfq == 'Moderately agree' ] <- 4
mfq[mfq == 'Strongly agree' ] <- 5
# compute results
colnames(mfq) <- substr(colnames(mfq), 5, 7) 
names(mfq)[1] <- 'subID'
mfq <- mfq[-1,]
mfq[,2:33] <- as.numeric(unlist(mfq[,2:33]))
mfq <- mfq %>% mutate(careScore = rowSums(select(., c('1','7','12','17','23','28')), na.rm = TRUE),
                     fairnessScore = rowSums(select(., c('2','8','13','18','24','29')), na.rm = TRUE),
                     loyaltyScore = rowSums(select(., c('3','9','14','19','25','30')), na.rm = TRUE),
                     authorityScore = rowSums(select(., c('4','10','15','20','26','31')), na.rm = TRUE),
                     purityScore = rowSums(select(., c('5','11','16','21','27','32')), na.rm = TRUE))
hist(mfq$careScore)
hist(mfq$fairnessScore)
hist(mfq$loyaltyScore)
hist(mfq$authorityScore)
hist(mfq$purityScore)


# score disgust_scale
#part one
disgust_scale[disgust_scale == 'Strongly disagree' ] <- 0
disgust_scale[disgust_scale == 'Mildly disagree' ] <- 1
disgust_scale[disgust_scale == 'Neither agree nor disagree' ] <- 2
disgust_scale[disgust_scale == 'Mildly agree' ] <- 3
disgust_scale[disgust_scale == 'Strongly agree' ] <- 4
#part two
disgust_scale[disgust_scale == 'Not disgusting at all' ] <- 0
disgust_scale[disgust_scale == 'Slightly disgusting' ] <- 1
disgust_scale[disgust_scale == 'Moderately disgusting' ] <- 2
disgust_scale[disgust_scale == 'Very disgusting' ] <- 3
disgust_scale[disgust_scale == 'Extremely disgusting' ] <- 4
# compute results
colnames(disgust_scale) <- as.character(c('subID', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27)) 
disgust_scores <- select(disgust_scale, -'12', -'16') #get rid of catch questions
disgust_scores <- as.data.frame(disgust_scores[-1,]) 
disgust_scores[,2:25] <- as.numeric(unlist(disgust_scores[,2:25]))
reverse4 <- function(x) 4 - x
disgust_scores[,c('1','6','10')] <- reverse4(disgust_scores[,c('1','6','10')])
disgust_scores$score <- rowSums(disgust_scores[2:25])
hist(disgust_scores$score)


# score iri
iri <- as.data.frame(sapply(iri, function(s) substr(s, 1, 2)), stringsAsFactors = FALSE)
colnames(iri) <- substr(colnames(iri), 5, 7) 
iri <- as.data.frame(sapply(iri, function(x) gsub("^\t* *\"* *| *\"* *\t*$", "", x)), stringsAsFactors = FALSE)
names(iri)[1] <- 'subID'
iri <- iri[-1,]
iri[iri == 'A' ] <- 0
iri[iri == 'B' ] <- 1
iri[iri == 'C' ] <- 2
iri[iri == 'D' ] <- 3
iri[iri == 'E' ] <- 4
iri[,2:29] <- as.numeric(unlist(iri[,2:29]))
reverseScoredIRI <- c('3', '4', '7', '12', '13', '14', '15', '18', '19')
iri[,reverseScoredIRI] <- reverse4(iri[,reverseScoredIRI])
#subscale coding
perspective_taking <- c('3','8','11','15','21','25','28')
fantasy <- c('1', '5', '7','12','16','23','26')
empathic_concern <- c('2','4','9','14','18','20','22')
personal_distress <- c('6','10','13','17', '19', '24','27')
#compute scores
iri <- iri %>% mutate(PerspectiveTakingScore = rowSums(select(., perspective_taking), na.rm = TRUE),
                     FantasyScore = rowSums(select(., fantasy), na.rm = TRUE),
                     EmpathicConcernScore = rowSums(select(., empathic_concern), na.rm = TRUE),
                     PersonalDistressScore = rowSums(select(., personal_distress), na.rm = TRUE))

# explore spread
hist(iri$PerspectiveTakingScore)
hist(iri$FantasyScore)
hist(iri$EmpathicConcernScore)
hist(iri$PersonalDistressScore)


# score secs - conservatism increases with score
colnames(secs) <- substr(colnames(secs), 6, 8) 
names(secs)[1] <- 'subID'
secs <- secs[-1,]
secs[,2:13] <- as.numeric(unlist(secs[,2:13]))
social <- c('1','3','4','7','8','11','12')
economic <- c('2', '5', '6', '9', '10')
reverse100 <- function(x) 100 - x
reverseScoredSECS <- c('1', '5')
secs[,reverseScoredSECS] <- reverse100(secs[,reverseScoredSECS])
secs <- secs %>% mutate(SocialConservatism = rowMeans(select(., social), na.rm = TRUE),
                       EconomicConservatism = rowMeans(select(., economic), na.rm = TRUE))
# explore spread
hist(secs$SocialConservatism)
count(secs, SocialConservatism >= 50)
hist(secs$EconomicConservatism)
count(secs, EconomicConservatism >= 50)


# tidy emorate
emorate[2:30, 2:721] <- as.data.frame(sapply(emorate[2:30, 2:721], function(s) substr(s, 1, 2)), stringsAsFactors = FALSE)
names(emorate)[1] <- 'subID'
emorate <- emorate %>% separate()

```

# Memory Accuracy
## Corrected recognition
```{r, fig.width=10, fig.height=4}

corrected_recognition <- ret_all %>%
                  group_by(subID, category) %>%
                  count(SDT) %>%
                  pivot_wider(names_from = SDT, values_from = n) %>%
                  replace_na(list(correctRejection=0, falseAlarm=0, hit=0, miss=0)) %>%
                  summarise(hitRate = hit/(hit + miss),
                            FARate = falseAlarm /(falseAlarm + correctRejection),
                            nOld = sum(hit) + sum(miss),
                            nNew = sum(falseAlarm) + sum(correctRejection),
                            corrected_recognition= hitRate - FARate)

CR_mean <- corrected_recognition %>%
           group_by(category) %>%
           summarise(n = length(corrected_recognition),
                     variance = var(corrected_recognition, na.rm = TRUE),
                     corrected_recognition = mean(corrected_recognition)
                     )
print(kable(CR_mean, caption = 'Corrected recognition by moral foundation'))


# order foundations by corrected recognition accuracy
corrected_recognition$category <- factor(corrected_recognition$category, 
                                levels=c('Authority', 'Loyalty', 'Liberty', 'Care-Phys', 'Social Norms', 'Fairness', 'Care-Emo', 'Purity'))


# plot
p <- ggplot(data = corrected_recognition, aes(x = category, y = corrected_recognition, fill=category, color=category)) +
     geom_dotplot(size = 0.01, binaxis='y', stackdir='center', binwidth = 0.02) +
     stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
     stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                  width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
     ggtitle('Corrected Recognition by Moral Foundation (n=28)')
     #geom_line(aes(group=subID, color=NA, fill=NA))
print(p)      


# with subset of ppts with encoding + retrieval 
corrected_recognition_sub <- data %>%
                  group_by(subID, category) %>%
                  count(SDT) %>%
                  pivot_wider(names_from = SDT, values_from = n) %>%
                  replace_na(list(correctRejection=0, falseAlarm=0, hit=0, miss=0)) %>%
                  summarise(hitRate = hit/(hit + miss),
                            FARate = falseAlarm /(falseAlarm + correctRejection),
                            nOld = sum(hit) + sum(miss),
                            nNew = sum(falseAlarm) + sum(correctRejection),
                            corrected_recognition= hitRate - FARate)

# order foundations by corrected recognition accuracy
corrected_recognition_sub$category <- factor(corrected_recognition_sub$category, 
                                levels=c('Authority', 'Loyalty', 'Liberty', 'Care-Phys', 'Social Norms', 'Fairness', 'Care-Emo', 'Purity'))

CR_sub_mean <- corrected_recognition_sub %>%
           group_by(category) %>%
           summarise(n = length(corrected_recognition_sub),
                     variance = var(corrected_recognition, na.rm = TRUE),
                     corrected_recognition = mean(corrected_recognition))

print(kable(CR_sub_mean, caption = 'Corrected recognition by moral foundation (n=25)'))

# plot
p <- ggplot(data = corrected_recognition_sub, aes(x = category, y = corrected_recognition, fill=category, color=category)) +
     geom_dotplot(size = 0.01, binaxis='y', stackdir='center', binwidth = 0.02) +
     stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
     stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                  width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
     ggtitle('Corrected Recognition by Moral Foundation (n=25)')
     #geom_line(aes(group=subID, color=NA, fill=NA))
print(p)  

# facet accuracy by individual moral rating
moral_memory <- subset(data, old == 1) %>%
                group_by(subID, moral_decision) %>%
                summarise(prop_correct = mean(hits, na.rm = TRUE)) %>%
                filter(!is.na(moral_decision))

moral_memory_group_mean <- moral_memory %>%
                           group_by(moral_decision) %>%
                           summarise(group_avg = mean(prop_correct))

p <- ggplot(data = moral_memory, aes(x = moral_decision, y = prop_correct, fill=moral_decision, color=moral_decision)) +
      geom_dotplot(size = 0.01, binaxis='y', stackdir='center', binwidth = 0.02) +
      stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
      stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                   width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
      theme(aspect.ratio = 4/4) +
ggtitle('Proportion of Hits by Moral Judgment (n=25)')
print(p) 

# Correceted recognition by confidence
corrected_recognition_conf <- ret_all %>%
                   group_by(subID, confresp) %>%
                   count(SDT) %>%
                   pivot_wider(names_from = SDT, values_from = n) %>%
                   replace_na(list(correctRejection=0, falseAlarm=0, hit=0, miss=0)) %>%
                   summarise(hitRate = hit/(hit + miss),
                             FARate = falseAlarm /(falseAlarm + correctRejection),
                             nOld = sum(hit) + sum(miss),
                             nNew = sum(falseAlarm) + sum(correctRejection),
                             corrected_recognition= hitRate - FARate) %>%
                             mutate_at(vars(hitRate, FARate, corrected_recognition), ~na_if(., 'NaN'))
 
 CR_conf_mean <- corrected_recognition_conf %>%
            group_by(confresp) %>%
            summarise(n = length(corrected_recognition),
                      variance = var(corrected_recognition, na.rm = TRUE),
                      avg_cr = mean(corrected_recognition, na.rm = TRUE))
 
# print(kable(CR_conf_mean, caption = 'Corrected recognition by moral foundation & confidence'))
 
p <- ggplot(data = corrected_recognition_conf, aes(x = confresp, y = corrected_recognition, fill=confresp, color=confresp)) +
      geom_dotplot(size = 0.02, binaxis='y', stackdir='center', binwidth = 0.02) +
      stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
      stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                   width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
      theme(aspect.ratio = 4/4) +
      ggtitle('Corrected Recognition by Confidence (n=28)')
      print(p)     
      
# facet accuracy by moral decision (low vs. high)
# moral_memory_accuracy <- subset(data, old == 1)
# moral_memory_accuracy$moral_resp[moral_memory_accuracy$moral_decision == 1 |moral_memory_accuracy$moral_decision == 2] = 'fine'
# moral_memory_accuracy$moral_resp[moral_memory_accuracy$moral_decision == 3 |moral_memory_accuracy$moral_decision == 4] = 'wrong'
# 
# moral_memory_accuracy <- moral_memory_accuracy %>%
#                          group_by(subID, category, moral_resp) %>%
#                          summarise(prop_correct = mean(hits, na.rm = TRUE))
# 
# moral_memory_accuracy <- filter(moral_memory_accuracy, !is.na(moral_resp))
# 
# p <- ggplot(data = moral_memory_accuracy, aes(x = category, y = prop_correct, fill=category, color=category)) +
#       geom_dotplot(size = 0.01, binaxis='y', stackdir='center', binwidth = 0.02) +
#       stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
#       stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
#                    width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
#       facet_grid(cols = vars(moral_resp)) +
# ggtitle('Proportion of Hits by Moral Judgment')
# print(p)        

```

# Memory Confidence
```{r, fig.width=10, fig.height=4}

# Accuracy Independent
confidence <- ret_all %>%
                  group_by(subID, category) %>%
                  summarise(PropHigh = mean(high_conf))

confidence_mean <- confidence %>%
           group_by(category) %>%
           summarise(group_conf = mean(PropHigh), SD = sd(PropHigh))
print(kable(confidence_mean, caption = 'Accuracy-Independent Confidence (n=28)'))

# order foundations by corrected recognition accuracy
confidence$category <- factor(confidence$category, 
                                levels=c('Authority', 'Loyalty', 'Liberty', 'Care-Phys', 'Social Norms', 'Fairness', 'Care-Emo', 'Purity'))

p <- ggplot(data = confidence, aes(x = category, y = PropHigh, fill=category, color=category)) +
     geom_dotplot(size = 0.01, binaxis='y', stackdir='center', binwidth = 0.02) +
     stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
     stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                  width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
     ggtitle('Accuracy-Independent Memory Confidence by Moral Foundation (n=28)')
print(p)

## subset of ret & encoding
confidence_sub <- data %>%
                  group_by(subID, category) %>%
                  summarise(PropHigh = mean(high_conf))

confidence_mean <- confidence_sub %>%
           group_by(category) %>%
           summarise(group_conf = mean(PropHigh), SD = sd(PropHigh))
print(kable(confidence_mean, caption = 'Accuracy-Independent Confidence (n=28)'))

# order foundations by corrected recognition accuracy
confidence_sub$category <- factor(confidence_sub$category, 
                                levels=c('Authority', 'Loyalty', 'Liberty', 'Care-Phys', 'Social Norms', 'Fairness', 'Care-Emo', 'Purity'))

p <- ggplot(data = confidence_sub, aes(x = category, y = PropHigh, fill=category, color=category)) +
     geom_dotplot(size = 0.01, binaxis='y', stackdir='center', binwidth = 0.02) +
     stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
     stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                  width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
     ggtitle('Accuracy-Independent Memory Confidence by Moral Foundation (n=25)')
print(p) 



# Hits Confidence
hits_confidence <- subset(ret_all, correct == 1) %>%
                  group_by(subID, category) %>%
                  summarise(PropHigh = mean(high_conf))

hits_confidence_mean <- hits_confidence %>%
                                 group_by(category) %>%
                                 summarise(group_conf = mean(PropHigh), SD = sd(PropHigh))

print(kable(hits_confidence_mean, caption = 'Confidence: Hits & Correct Rejections'))

# order foundations by corrected recognition accuracy
confidence$category <- factor(confidence$category, 
                                levels=c('Authority', 'Loyalty', 'Liberty', 'Care-Phys', 'Social Norms', 'Fairness', 'Care-Emo', 'Purity'))

p <- ggplot(data = hits_confidence, aes(x = category, y = PropHigh, fill=category, color=category)) +
     geom_dotplot(size = 0.01, binaxis='y', stackdir='center', binwidth = 0.02) +
     stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
     stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                  width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
     ggtitle('Confidence: Hits & Correct Rejections (n=28)')
print(p) 

# Misses Confidence
misses_confidence <- subset(ret_all, correct == 0) %>%
                  group_by(subID, category) %>%
                  summarise(PropHigh = mean(high_conf))

misses_confidence_mean <- misses_confidence %>%
                                 group_by(category) %>%
                                 summarise(group_conf = mean(PropHigh), SD = sd(PropHigh))

print(kable(misses_confidence_mean, caption = 'Confidence: Hits & Correct Rejections'))

# order foundations by recognition accuracy
misses_confidence$category <- factor(misses_confidence$category, 
                                levels=c('Authority', 'Loyalty', 'Liberty', 'Care-Phys', 'Social Norms', 'Fairness', 'Care-Emo', 'Purity'))

p <- ggplot(data = misses_confidence, aes(x = category, y = PropHigh, fill=category, color=category)) +
     geom_dotplot(size = 0.01, binaxis='y', stackdir='center', binwidth = 0.02) +
     stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
     stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                  width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
     ggtitle('Confidence: Misses & False Alarms (n=28)')
print(p) 

```

# Correlation between Accuracy and Moral Judgment
``` {r, fig.width=15, fig.height=4}

data$moral_decision[data$moral_decision == 'NaN'] = NA

# points = participant estimates
moral_memory_corr <- subset(data, old == 1) %>%
                group_by(subID, category) %>%
                summarise(correlation = fisherz(cor(hits, as.numeric(moral_decision), use = "pairwise.complete.obs")))

# order by increasing recognition accuracy
moral_memory_corr$category <- factor(moral_memory_corr$category, 
                                levels=c('Authority', 'Loyalty', 'Liberty', 'Care-Phys', 'Social Norms', 'Fairness', 'Care-Emo', 'Purity'))

p <- ggplot(data = moral_memory_corr, aes(x = category, y = correlation, fill=category, color=category)) +
     geom_dotplot(size = 0.01, binaxis='y', stackdir='center', binwidth = 0.02) +
     stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
     stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                  width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
     ggtitle('Correlation between Proportion of Hits & Moral Decision (n=25)')
print(p)


# averaged across participants
moral_memory_corr_mean <- moral_memory_corr %>%
                     group_by(category) %>%
                     summarise(avg_correlation = mean(correlation, na.rm = TRUE))

# order by increasing recognition accuracy
moral_memory_corr_mean$category <- factor(moral_memory_corr_mean$category, 
                                levels=c('Authority', 'Loyalty', 'Liberty', 'Care-Phys', 'Social Norms', 'Fairness', 'Care-Emo', 'Purity'))

p <- ggplot(data = moral_memory_corr_mean, aes(x = category, y = avg_correlation, fill=category, color=category)) +
     stat_summary(fun.y = mean, geom = 'bar', alpha = 0.5, position = position_dodge(1)) +
     stat_summary(fun.data = mean_se, geom = 'errorbar', fun.args = list(mult = 1.96), 
                  width = 0.2, color = 'black', size = 0.3, position = position_dodge(1))+
     ggtitle('Hits & Moral Decision Averaged Across Participants')
print(p)



```

# Generalized Linear Mixed Model
``` {r, fig.width=15, fig.height=8}

## some handy commands
# exp(fixef(modelOld1)): transforms model output from log likelihood to odds ratio
# anova(modelOld1, modelOld2): use for model comparisons [**don't look at the output of either model; use the results of this comparison to guide your result-seeking**]
# emmeans(modelNew, pairwise ~ category): gives you marginal means for each level of the fixed effect factor & pairwise comparisons among levels of the factor
# emmeans(modelNew, 'category', type='response'): gives you probabilities for each level of the fixed effect factor
# emmeans(modelOld1, pairwise ~ category, lmer.df = "satterthwaite"): gives you probabilities for each fixed effect and odds ratios + p-values for pairwise comparisons among levels of the fixed effect factor


ret_old <- subset(data, old == 1)
ret_new <- subset(data, old == 0)

ret_old$confresp <- as.factor(as.numeric(ret_old$confresp))
ret_new$confresp <- as.factor(as.numeric(ret_new$confresp))

# predicting correct using category and moral decision
modelOld1 <- glmer(correct ~ moral_decision + category + confresp + (1 + moral_decision + confresp | subID),
               data=ret_old, family=binomial(link = 'logit'), control=glmerControl(optCtrl=list(maxfun=25000)))
summary(modelOld1)
plot(modelOld1)

               data=ret_old, family=binomial(link = 'logit'), control=glmerControl(optCtrl=list(maxfun=25000)))
summary(modelOld2)
modelOld2 <- glmer(correct ~ moral_decision + category  + (1 + category | subID),
plot(modelOld2)
anova(modelOld1, modelOld2)

modelOld <- glmer(correct ~ moral_decision * category * confresp + (1 + category | subID),
               data=ret_old, family=binomial(link = 'logit'),control=glmerControl(optCtrl=list(maxfun=50000)))
summary(modelOld)
plot(modelOld)

# plot predictions
modelOld %>% ggpredict(terms=c('moral_decision', 'category')) %>%
  ggplot(aes(x=interaction(x, group), y=predicted, group=group, fill=group)) + geom_col() + geom_errorbar(aes(ymax=conf.high, ymin=conf.low)) + ggtitle('Predicted accuracy (proportion correct) n=25')

modelOld %>% ggpredict(terms=c('moral_decision')) %>%
  ggplot(aes(x=x, y=predicted, fill=x)) + geom_col() + geom_errorbar(aes(ymax=conf.high, ymin=conf.low)) + ggtitle('Predicted accuracy (proportion correct) n=25')


# model %>% ggpredict(terms=c('category')) %>%
#   ggplot(aes(x=x, y=predicted)) + geom_col() + geom_errorbar(aes(ymax=conf.high, ymin=conf.low))
# model %>% ggpredict(terms=c('old')) %>%
#   ggplot(aes(x=x, y=predicted)) + geom_col() + geom_errorbar(aes(ymax=conf.high, ymin=conf.low))

# predicting correct using category 
modelNew <- glmer(correct ~ category * confresp + (1 + category | subID), data=ret_new, family=binomial(link = 'logit'), control=glmerControl(optCtrl=list(maxfun=50000)))
summary(modelNew)
plot(modelNew)

modelNew %>% ggpredict(terms=c('category')) %>%
  ggplot(aes(x=x, y=predicted)) + geom_col() + geom_errorbar(aes(ymax=conf.high, ymin=conf.low)) + ggtitle('Predicted accuracy (proportion correct) n=25')


```